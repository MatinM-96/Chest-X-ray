{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7f0a792-8882-4623-be7b-10f035c2ef4a",
   "metadata": {},
   "source": [
    "this is the first stage, we train 20 first epock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92163cdd-3658-401a-9268-75032fe6da63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "df shape: (112120, 12)\n",
      "Y shape: (112120, 14)\n",
      "Positives per class: {'Atelectasis': 11559, 'Cardiomegaly': 2776, 'Effusion': 13317, 'Infiltration': 19894, 'Mass': 5782, 'Nodule': 6331, 'Pneumonia': 1431, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Edema': 2303, 'Emphysema': 2516, 'Fibrosis': 1686, 'Pleural_Thickening': 3385, 'Hernia': 227}\n",
      "Split sizes -> Train: 89618 Val: 11254 Test: 11248\n",
      "Indexed files: 112120\n",
      "Missing among first 20: 0\n",
      "DL sizes -> 89618 11254 11248\n",
      " Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 2801/2801 [18:29<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.7545 | microF1=0.1608 | macroF1=0.1584 | train_loss=1.1880\n",
      " Saved last: densenet121_last.pt (epoch=1)\n",
      " Saved BEST: densenet121_best.pt (epoch=1, best_auc=0.7545)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 2801/2801 [18:25<00:00,  2.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.7721 | microF1=0.2022 | macroF1=0.1865 | train_loss=1.1045\n",
      " Saved last: densenet121_last.pt (epoch=2)\n",
      " Saved BEST: densenet121_best.pt (epoch=2, best_auc=0.7721)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 2801/2801 [18:23<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.7952 | microF1=0.1951 | macroF1=0.1844 | train_loss=1.0546\n",
      " Saved last: densenet121_last.pt (epoch=3)\n",
      " Saved BEST: densenet121_best.pt (epoch=3, best_auc=0.7952)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 2801/2801 [18:22<00:00,  2.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8222 | microF1=0.2222 | macroF1=0.2090 | train_loss=1.0119\n",
      " Saved last: densenet121_last.pt (epoch=4)\n",
      " Saved BEST: densenet121_best.pt (epoch=4, best_auc=0.8222)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 2801/2801 [18:18<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8234 | microF1=0.2365 | macroF1=0.2163 | train_loss=0.9701\n",
      " Saved last: densenet121_last.pt (epoch=5)\n",
      " Saved BEST: densenet121_best.pt (epoch=5, best_auc=0.8234)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 2801/2801 [19:28<00:00,  2.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8335 | microF1=0.2389 | macroF1=0.2078 | train_loss=0.9354\n",
      " Saved last: densenet121_last.pt (epoch=6)\n",
      " Saved BEST: densenet121_best.pt (epoch=6, best_auc=0.8335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 2801/2801 [20:29<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8391 | microF1=0.2633 | macroF1=0.2360 | train_loss=0.8987\n",
      " Saved last: densenet121_last.pt (epoch=7)\n",
      " Saved BEST: densenet121_best.pt (epoch=7, best_auc=0.8391)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 2801/2801 [20:26<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8439 | microF1=0.2698 | macroF1=0.2383 | train_loss=0.8618\n",
      " Saved last: densenet121_last.pt (epoch=8)\n",
      " Saved BEST: densenet121_best.pt (epoch=8, best_auc=0.8439)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 2801/2801 [19:47<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8462 | microF1=0.2718 | macroF1=0.2405 | train_loss=0.8357\n",
      " Saved last: densenet121_last.pt (epoch=9)\n",
      " Saved BEST: densenet121_best.pt (epoch=9, best_auc=0.8462)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 2801/2801 [18:29<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8466 | microF1=0.2741 | macroF1=0.2429 | train_loss=0.8218\n",
      " Saved last: densenet121_last.pt (epoch=10)\n",
      " Saved BEST: densenet121_best.pt (epoch=10, best_auc=0.8466)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 2801/2801 [18:38<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8465 | microF1=0.2738 | macroF1=0.2417 | train_loss=0.8131\n",
      " Saved last: densenet121_last.pt (epoch=11)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 2801/2801 [18:34<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8479 | microF1=0.2688 | macroF1=0.2396 | train_loss=0.8137\n",
      " Saved last: densenet121_last.pt (epoch=12)\n",
      " Saved BEST: densenet121_best.pt (epoch=12, best_auc=0.8479)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 2801/2801 [18:31<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8468 | microF1=0.2701 | macroF1=0.2417 | train_loss=0.8159\n",
      " Saved last: densenet121_last.pt (epoch=13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:  88%|████████▊ | 2464/2801 [16:18<02:13,  2.52it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 213\u001b[0m\n\u001b[1;32m    211\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    212\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    214\u001b[0m     xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m), yb\u001b[38;5;241m.\u001b[39mto(DEVICE, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    215\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].apply(lambda x: hash(x) % 10)  # 0..9\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Your layout: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 32\n",
    "NUM_WORKERS = 2  # set 0 if multiprocessing issues\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight from TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-class positives in TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # avoid div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION ==========================\n",
    "def evaluate(model, loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb))\n",
    "            ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per class\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    # F1 at fixed threshold (reference only)\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "    return mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = True\n",
    "EPOCHS    = 20\n",
    "\n",
    "def make_ckpt(epoch, best_auc):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Resume\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Loop\n",
    "for ep in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\"):\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate(model, val_dl)\n",
    "    print(f\"[Val] mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "    # Always save \"last\"\n",
    "    save_last(ep, best_auc)\n",
    "\n",
    "    # Save \"best\" if improved\n",
    "    if val_mean_auc > best_auc:\n",
    "        best_auc = val_mean_auc\n",
    "        save_best(ep, best_auc)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb24e892-1d95-4689-9b8b-4552902c269e",
   "metadata": {},
   "source": [
    "the model  start, lear well, but after some epock the larning whill more stabil, the reasen should be that the model is overfitting and not learn the data, \n",
    "to find out the reasn that the  model is not learning we did stop the model, we want to continue the with the model to incre the accurant.  some of the acurrancy is stable it can be noise data, or some lables that re in the traing sett that we need to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad51589a-9271-4efc-938c-0a9033b928d1",
   "metadata": {},
   "source": [
    "Second training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eee334-fa0b-4986-91d9-50fad454c364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "df shape: (112120, 12)\n",
      "Y shape: (112120, 14)\n",
      "Positives per class: {'Atelectasis': 11559, 'Cardiomegaly': 2776, 'Effusion': 13317, 'Infiltration': 19894, 'Mass': 5782, 'Nodule': 6331, 'Pneumonia': 1431, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Edema': 2303, 'Emphysema': 2516, 'Fibrosis': 1686, 'Pleural_Thickening': 3385, 'Hernia': 227}\n",
      "Split sizes -> Train: 89425 Val: 11395 Test: 11300\n",
      "Indexed files: 112120\n",
      "Missing among first 20: 0\n",
      "DL sizes -> 89425 11395 11300\n",
      " Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 1398/1398 [20:19<00:00,  1.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.7883 | microF1=0.2181 | macroF1=0.1903 | train_loss=1.0652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 1398/1398 [20:02<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8035 | microF1=0.2343 | macroF1=0.2136 | train_loss=0.9594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 1398/1398 [20:05<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8098 | microF1=0.2514 | macroF1=0.2212 | train_loss=0.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 1398/1398 [20:26<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8279 | microF1=0.2603 | macroF1=0.2262 | train_loss=0.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 1398/1398 [20:06<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8292 | microF1=0.2761 | macroF1=0.2422 | train_loss=0.8280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 1398/1398 [19:24<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8282 | microF1=0.2858 | macroF1=0.2425 | train_loss=0.7821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 1398/1398 [18:19<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8353 | microF1=0.2790 | macroF1=0.2469 | train_loss=0.7376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 1398/1398 [18:23<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8355 | microF1=0.2835 | macroF1=0.2500 | train_loss=0.6955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 1398/1398 [18:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8357 | microF1=0.2870 | macroF1=0.2543 | train_loss=0.6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 1398/1398 [18:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8351 | microF1=0.3004 | macroF1=0.2622 | train_loss=0.6445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 1398/1398 [18:18<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8351 | microF1=0.3004 | macroF1=0.2653 | train_loss=0.6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 1398/1398 [18:17<00:00,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Val] mean AUROC=0.8347 | microF1=0.3006 | macroF1=0.2615 | train_loss=0.6386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:  33%|███▎      | 462/1398 [06:04<10:51,  1.44it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].apply(lambda x: hash(x) % 10)  # 0..9\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Your layout: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 2  # set 0 if multiprocessing issues\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight from TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-class positives in TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # avoid div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION ==========================\n",
    "def evaluate(model, loader, threshold=0.5):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            p = torch.sigmoid(model(xb))\n",
    "            ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per class\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    # F1 at fixed threshold (reference only)\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "    return mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = False\n",
    "EPOCHS    = 20\n",
    "\n",
    "def make_ckpt(epoch, best_auc):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Resume\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Loop\n",
    "for ep in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\"):\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    scheduler.step()\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate(model, val_dl)\n",
    "    print(f\"[Val] mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "    # # Always save \"last\"\n",
    "    # save_last(ep, best_auc)\n",
    "\n",
    "    # # Save \"best\" if improved\n",
    "    # if val_mean_auc > best_auc:\n",
    "    #     best_auc = val_mean_auc\n",
    "    #     save_best(ep, best_auc)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461e5a66-8fe3-47e8-93f6-5cb1d739bb42",
   "metadata": {},
   "source": [
    "even that i incereas the modell to the 100 epch with the batch isze of 64, the result didnt get beder then 84, which we reach the same result with the previpus one. the under code we change the leraning rate fraom the 1 to 5, with a bache size of 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f55912c1-6a31-4aa5-a2f3-68a9c7d1f896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "df shape: (112120, 12)\n",
      "Y shape: (112120, 14)\n",
      "Positives per class: {'Atelectasis': 11559, 'Cardiomegaly': 2776, 'Effusion': 13317, 'Infiltration': 19894, 'Mass': 5782, 'Nodule': 6331, 'Pneumonia': 1431, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Edema': 2303, 'Emphysema': 2516, 'Fibrosis': 1686, 'Pleural_Thickening': 3385, 'Hernia': 227}\n",
      "Split sizes -> Train: 90000 Val: 10437 Test: 11683\n",
      "Indexed files: 112120\n",
      "Missing among first 20: 0\n",
      "DL sizes -> 90000 10437 11683\n",
      " Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: 100%|██████████| 1407/1407 [18:46<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.7708 | microF1=0.1898 | macroF1=0.1770 | train_loss=1.1624 | Val_loss=1.1219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: 100%|██████████| 1407/1407 [18:52<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.7855 | microF1=0.1836 | macroF1=0.1792 | train_loss=1.0800 | Val_loss=1.0818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: 100%|██████████| 1407/1407 [18:46<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.7948 | microF1=0.2276 | macroF1=0.2015 | train_loss=1.0422 | Val_loss=1.0636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: 100%|██████████| 1407/1407 [18:50<00:00,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8105 | microF1=0.2391 | macroF1=0.2179 | train_loss=1.0062 | Val_loss=1.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: 100%|██████████| 1407/1407 [18:48<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8117 | microF1=0.2382 | macroF1=0.2091 | train_loss=0.9862 | Val_loss=1.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: 100%|██████████| 1407/1407 [18:37<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8099 | microF1=0.2144 | macroF1=0.2057 | train_loss=0.9679 | Val_loss=1.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: 100%|██████████| 1407/1407 [18:46<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8199 | microF1=0.2285 | macroF1=0.2077 | train_loss=0.9497 | Val_loss=0.9816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: 100%|██████████| 1407/1407 [18:44<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8226 | microF1=0.2422 | macroF1=0.2179 | train_loss=0.9276 | Val_loss=0.9955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: 100%|██████████| 1407/1407 [18:40<00:00,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8273 | microF1=0.2342 | macroF1=0.2116 | train_loss=0.9181 | Val_loss=0.9579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: 100%|██████████| 1407/1407 [18:44<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8292 | microF1=0.2622 | macroF1=0.2335 | train_loss=0.9055 | Val_loss=0.9853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: 100%|██████████| 1407/1407 [18:44<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8103 | microF1=0.2008 | macroF1=0.1957 | train_loss=0.8920 | Val_loss=1.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: 100%|██████████| 1407/1407 [18:43<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean AUROC=0.8301 | microF1=0.2433 | macroF1=0.2231 | train_loss=0.8833 | Val_loss=0.9483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: 100%|██████████| 1407/1407 [18:47<00:00,  1.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR now: 0.000500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 326\u001b[0m\n\u001b[1;32m    320\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_ds)\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate(model, val_dl)\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# print(f\"[Val] mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f}\")\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m mean AUROC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mean_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | microF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_micro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | macroF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_macro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | train_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;66;03m# # Always save \"last\"\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;66;03m# save_last(ep, best_auc)\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m \n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# ========================== 11) TEST ==========================\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 221\u001b[0m, in \u001b[0;36mevaluate_with_loss\u001b[0;34m(model, loader, criterion, threshold)\u001b[0m\n\u001b[1;32m    218\u001b[0m ys, ps \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    222\u001b[0m         xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(DEVICE), yb\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    223\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(xb)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].apply(lambda x: hash(x) % 10)  # 0..9\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Your layout: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 2  # set 0 if multiprocessing issues\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight from TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-class positives in TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # avoid div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION ==========================\n",
    "# def evaluate(model, loader, threshold=0.5):\n",
    "#     model.eval()\n",
    "#     ys, ps = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             p = torch.sigmoid(model(xb))\n",
    "#             ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "#     ys = torch.cat(ys, 0).numpy()\n",
    "#     ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "#     # AUROC per class\n",
    "#     aurocs = []\n",
    "#     for c in range(len(LABELS)):\n",
    "#         y_c, p_c = ys[:, c], ps[:, c]\n",
    "#         try:\n",
    "#             aurocs.append(roc_auc_score(y_c, p_c))\n",
    "#         except ValueError:\n",
    "#             aurocs.append(np.nan)\n",
    "#     mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "#     # F1 at fixed threshold (reference only)\n",
    "#     preds = (ps >= threshold).astype(\"int32\")\n",
    "#     micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "#     macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "#     return mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 9b) EVALUATION WITH LOSS ==========================\n",
    "# def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "#     model.eval()\n",
    "#     ys, ps = [], []\n",
    "#     val_loss = 0.0\n",
    "#     n = 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#             val_loss += loss.item() * xb.size(0)\n",
    "#             n += xb.size(0)\n",
    "#             p = torch.sigmoid(logits)\n",
    "#             ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "\n",
    "#     val_loss /= max(n, 1)\n",
    "\n",
    "#     ys = torch.cat(ys, 0).numpy()\n",
    "#     ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "#     # AUROC per class\n",
    "#     aurocs = []\n",
    "#     for c in range(len(LABELS)):\n",
    "#         y_c, p_c = ys[:, c], ps[:, c]\n",
    "#         try:\n",
    "#             aurocs.append(roc_auc_score(y_c, p_c))\n",
    "#         except ValueError:\n",
    "#             aurocs.append(np.nan)\n",
    "#     mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "#     # F1 at fixed threshold (reference only)\n",
    "#     preds = (ps >= threshold).astype(\"int32\")\n",
    "#     micro_f1 = f1_score(ys, preds, average=\"micro\",  zero_division=0)\n",
    "#     macro_f1 = f1_score(ys, preds, average=\"macro\",  zero_division=0)\n",
    "\n",
    "#     return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 9b) EVALUATION WITH LOSS ==========================\n",
    "def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "    ys, ps = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            # Viktig: loss på LOGITS (ikke sigmoid), samme criterion som trening\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n    += xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            ys.append(yb.cpu()); ps.append(probs.cpu())\n",
    "\n",
    "    val_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per klasse\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    # F1 ved fast terskel (referanse)\n",
    "    from sklearn.metrics import f1_score\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = False\n",
    "EPOCHS    = 100\n",
    "\n",
    "def make_ckpt(epoch, best_auc):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Resume\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Loop\n",
    "for ep in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\"):\n",
    "        xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer); scaler.update()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    # scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "    # Etter scheduler.step():\n",
    "    curr_lr = optimizer.param_groups[0][\"lr\"]\n",
    "    print(f\"LR now: {curr_lr:.6f}\")\n",
    "\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    # val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate(model, val_dl)\n",
    "    # print(f\"[Val] mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate_with_loss(model, val_dl, criterion)\n",
    "    print(f\" mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f} | Val_loss={val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # # Always save \"last\"\n",
    "    # save_last(ep, best_auc)\n",
    "\n",
    "    # # Save \"best\" if improved\n",
    "    # if val_mean_auc > best_auc:\n",
    "    #     best_auc = val_mean_auc\n",
    "    #     save_best(ep, best_auc)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f88aa22-af47-4a33-a356-670bb54cf802",
   "metadata": {},
   "source": [
    "in the above we try did try to print the loss val for bedre training analytic and deactive the stackduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95b25a8-9130-4c54-8326-fd1357268ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stop når val_metric ikke forbedres innen 'patience' epoker.\n",
    "    mode='max' brukes for metrikker som AUROC/F1 (vi vil opp),\n",
    "    mode='min' for loss (vi vil ned).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=8, min_delta=0.0, mode='max', restore_best=True):\n",
    "        assert mode in ('max', 'min')\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "        self.should_stop = False\n",
    "        self.best_state = None\n",
    "\n",
    "    def _is_better(self, current, best):\n",
    "        if self.mode == 'max':\n",
    "            return current > best + self.min_delta\n",
    "        else:\n",
    "            return current < best - self.min_delta\n",
    "\n",
    "    def step(self, current, model=None):\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False  # ikke stoppe første gang\n",
    "\n",
    "        if self._is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            if self.num_bad >= self.patience:\n",
    "                self.should_stop = True\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "\n",
    "\n",
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].apply(lambda x: hash(x) % 10)  # 0..9\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Your layout: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 2  # set 0 if multiprocessing issues\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight from TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-class positives in TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # avoid div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION ==========================\n",
    "# def evaluate(model, loader, threshold=0.5):\n",
    "#     model.eval()\n",
    "#     ys, ps = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             p = torch.sigmoid(model(xb))\n",
    "#             ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "#     ys = torch.cat(ys, 0).numpy()\n",
    "#     ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "#     # AUROC per class\n",
    "#     aurocs = []\n",
    "#     for c in range(len(LABELS)):\n",
    "#         y_c, p_c = ys[:, c], ps[:, c]\n",
    "#         try:\n",
    "#             aurocs.append(roc_auc_score(y_c, p_c))\n",
    "#         except ValueError:\n",
    "#             aurocs.append(np.nan)\n",
    "#     mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "#     # F1 at fixed threshold (reference only)\n",
    "#     preds = (ps >= threshold).astype(\"int32\")\n",
    "#     micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "#     macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "#     return mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 9b) EVALUATION WITH LOSS ==========================\n",
    "# def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "#     model.eval()\n",
    "#     ys, ps = [], []\n",
    "#     val_loss = 0.0\n",
    "#     n = 0\n",
    "#     with torch.no_grad():\n",
    "#         for xb, yb in loader:\n",
    "#             xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#             val_loss += loss.item() * xb.size(0)\n",
    "#             n += xb.size(0)\n",
    "#             p = torch.sigmoid(logits)\n",
    "#             ys.append(yb.cpu()); ps.append(p.cpu())\n",
    "\n",
    "#     val_loss /= max(n, 1)\n",
    "\n",
    "#     ys = torch.cat(ys, 0).numpy()\n",
    "#     ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "#     # AUROC per class\n",
    "#     aurocs = []\n",
    "#     for c in range(len(LABELS)):\n",
    "#         y_c, p_c = ys[:, c], ps[:, c]\n",
    "#         try:\n",
    "#             aurocs.append(roc_auc_score(y_c, p_c))\n",
    "#         except ValueError:\n",
    "#             aurocs.append(np.nan)\n",
    "#     mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "#     # F1 at fixed threshold (reference only)\n",
    "#     preds = (ps >= threshold).astype(\"int32\")\n",
    "#     micro_f1 = f1_score(ys, preds, average=\"micro\",  zero_division=0)\n",
    "#     macro_f1 = f1_score(ys, preds, average=\"macro\",  zero_division=0)\n",
    "\n",
    "#     return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 9b) EVALUATION WITH LOSS ==========================\n",
    "def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "    ys, ps = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            # Viktig: loss på LOGITS (ikke sigmoid), samme criterion som trening\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n    += xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            ys.append(yb.cpu()); ps.append(probs.cpu())\n",
    "\n",
    "    val_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per klasse\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    # F1 ved fast terskel (referanse)\n",
    "    from sklearn.metrics import f1_score\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = False\n",
    "EPOCHS    = 100\n",
    "\n",
    "def make_ckpt(epoch, best_auc):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": scheduler.state_dict(),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc):\n",
    "    torch.save(make_ckpt(epoch, best_auc), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Resume\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Loop\n",
    "# for ep in range(start_epoch, EPOCHS + 1):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for xb, yb in tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\"):\n",
    "#         xb, yb = xb.to(DEVICE, non_blocking=True), yb.to(DEVICE, non_blocking=True)\n",
    "#         optimizer.zero_grad(set_to_none=True)\n",
    "#         with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "#             logits = model(xb)\n",
    "#             loss = criterion(logits, yb)\n",
    "#         scaler.scale(loss).backward()\n",
    "#         scaler.step(optimizer); scaler.update()\n",
    "#         running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "#     scheduler.step()\n",
    "\n",
    "\n",
    "\n",
    "# Stopper når val_mean_auc ikke forbedres i 'patience' epoker\n",
    "early_stopper = EarlyStopping(patience=8, min_delta=1e-3, mode='max', restore_best=True)\n",
    "best_auc = -1.0\n",
    "\n",
    "\n",
    "# oppsett\n",
    "    steps_per_epoch = len(train_dl)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, max_lr=5e-4, steps_per_epoch=steps_per_epoch, epochs=EPOCHS\n",
    "    )\n",
    "    \n",
    "    for ep in range(start_epoch, EPOCHS + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for xb, yb in train_dl:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "                logits = model(xb.to(DEVICE)); loss = criterion(logits, yb.to(DEVICE))\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "    \n",
    "            # <- VIKTIG: OneCycleLR steppes per batch\n",
    "            scheduler.step()\n",
    "    \n",
    "            running_loss += loss.item() * xb.size(0)\n",
    "        \n",
    "            train_loss = running_loss / len(train_ds)\n",
    "            # print gjeldende LR (fra første param group)\n",
    "            print(f\"LR now: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    # val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate(model, val_dl)\n",
    "    # print(f\"[Val] mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f}\")\n",
    "\n",
    "    \n",
    "    val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate_with_loss(model, val_dl, criterion)\n",
    "    print(f\" mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | train_loss={train_loss:.4f} | Val_loss={val_loss:.4f}\")\n",
    "\n",
    "\n",
    "    # # Always save \"last\"\n",
    "    # save_last(ep, best_auc)\n",
    "\n",
    "    # # Save \"best\" if improved\n",
    "    # if val_mean_auc > best_auc:\n",
    "    #     best_auc = val_mean_auc\n",
    "    #     save_best(ep, best_auc)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a61317ea-1673-400c-b670-678de9ec89e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "680236c3-95aa-435c-b773-6f710059f5ab",
   "metadata": {},
   "source": [
    "i did comment out the stack holder and did try to solve it without it, in this stage i implenet the early stoping for the traning and the did som change in the stach, see the above cod.\n",
    "the under code is an implemenation of the early stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a7c0e47-1ff5-4166-a6bf-2aeb3b31d7ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "df shape: (112120, 12)\n",
      "Y shape: (112120, 14)\n",
      "Positives per class: {'Atelectasis': 11559, 'Cardiomegaly': 2776, 'Effusion': 13317, 'Infiltration': 19894, 'Mass': 5782, 'Nodule': 6331, 'Pneumonia': 1431, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Edema': 2303, 'Emphysema': 2516, 'Fibrosis': 1686, 'Pleural_Thickening': 3385, 'Hernia': 227}\n",
      "Split sizes -> Train: 89644 Val: 11321 Test: 11155\n",
      "Indexed files: 112120\n",
      "Missing among first 20: 0\n",
      "DL sizes -> 89644 11321 11155\n",
      " Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 316\u001b[0m\n\u001b[1;32m    313\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_seen, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# --- VAL ---\u001b[39;00m\n\u001b[0;32m--> 316\u001b[0m val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_with_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mep\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m03d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    318\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m mean AUROC=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_mean_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | microF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_micro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | macroF1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_macro_f1\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    319\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | val_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# Early stop på AUROC\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 202\u001b[0m, in \u001b[0;36mevaluate_with_loss\u001b[0;34m(model, loader, criterion, threshold)\u001b[0m\n\u001b[1;32m    199\u001b[0m ys, ps \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 202\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xb, yb \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[1;32m    203\u001b[0m         xb, yb \u001b[38;5;241m=\u001b[39m xb\u001b[38;5;241m.\u001b[39mto(DEVICE), yb\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    204\u001b[0m         logits \u001b[38;5;241m=\u001b[39m model(xb)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1328\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1328\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1284\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1284\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1285\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1286\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:1132\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1119\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1120\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1129\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1132\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1134\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.10/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== EARLY STOPPING ==========================\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stop når val_metric ikke forbedres innen 'patience' epoker.\n",
    "    mode='max' brukes for metrikker som AUROC/F1 (vi vil opp),\n",
    "    mode='min' for loss (vi vil ned).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=8, min_delta=0.0, mode='max', restore_best=True):\n",
    "        assert mode in ('max', 'min')\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "        self.should_stop = False\n",
    "        self.best_state = None\n",
    "\n",
    "    def _is_better(self, current, best):\n",
    "        if self.mode == 'max':\n",
    "            return current > best + self.min_delta\n",
    "        else:\n",
    "            return current < best - self.min_delta\n",
    "\n",
    "    def step(self, current, model=None):\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False  # ikke stoppe første gang\n",
    "\n",
    "        if self._is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            if self.num_bad >= self.patience:\n",
    "                self.should_stop = True\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "def stable_bucket(pid: str) -> int:\n",
    "    return int(hashlib.md5(pid.encode()).hexdigest(), 16) % 10\n",
    "\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].map(stable_bucket)\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Layout forventet: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY  = (DEVICE == \"cuda\")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight fra TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-klasse positives i TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # unngå div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION HELPERS ==========================\n",
    "def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "    ys, ps = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n    += xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            ys.append(yb.cpu()); ps.append(probs.cpu())\n",
    "\n",
    "    val_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per klasse\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "# Wrapper så TEST-delen funker\n",
    "def evaluate(model, loader, threshold=0.5):\n",
    "    _, mean_auc, per_cls, micro_f1, macro_f1 = evaluate_with_loss(model, loader, criterion, threshold)\n",
    "    return mean_auc, per_cls, micro_f1, macro_f1\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = False\n",
    "EPOCHS    = 100\n",
    "\n",
    "def make_ckpt(epoch, best_auc, scheduler=None):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": (scheduler.state_dict() if scheduler is not None else None),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc, scheduler=None):\n",
    "    torch.save(make_ckpt(epoch, best_auc, scheduler), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc, scheduler=None):\n",
    "    torch.save(make_ckpt(epoch, best_auc, scheduler), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Opprett scheduler på forhånd\n",
    "steps_per_epoch = len(train_dl)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, steps_per_epoch=steps_per_epoch, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Resume (valgfritt)\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    if ckpt.get(\"scheduler_state\") is not None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Early stopping\n",
    "early_stopper = EarlyStopping(patience=8, min_delta=1e-3, mode='max', restore_best=True)\n",
    "\n",
    "for ep in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_seen = 0\n",
    "\n",
    "    for i, (xb, yb) in enumerate(tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\", leave=False)):\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # (valgfritt) gradient clipping:\n",
    "        # scaler.unscale_(optimizer)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()  # OneCycleLR per batch\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        n_seen += xb.size(0)\n",
    "\n",
    "    train_loss = running_loss / max(n_seen, 1)\n",
    "\n",
    "    # --- VAL ---\n",
    "    val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate_with_loss(model, val_dl, criterion)\n",
    "    print(f\"Epoch {ep:03d}/{EPOCHS} | \"\n",
    "          f\" mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | \"\n",
    "          f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Early stop på AUROC\n",
    "    if early_stopper.step(val_mean_auc, model=model):\n",
    "        print(f\"Early stopping at epoch {ep} (best={early_stopper.best:.4f})\")\n",
    "        if early_stopper.restore_best and early_stopper.best_state is not None:\n",
    "            model.load_state_dict(early_stopper.best_state)\n",
    "            model.to(DEVICE)\n",
    "        # valgfritt: lagre beste etter restore\n",
    "        save_best(ep, early_stopper.best, scheduler)\n",
    "        break\n",
    "\n",
    "    # Lagre \"last\" hver epoch\n",
    "    #save_last(ep, best_auc, scheduler)\n",
    "\n",
    "    # # Lagre \"best\" når forbedret\n",
    "    # if val_mean_auc > best_auc:\n",
    "    #     best_auc = val_mean_auc\n",
    "    #     save_best(ep, best_auc, scheduler)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(DEVICE)\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7d0a1c2-3523-476c-ba45-4b88a6167d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "df shape: (112120, 12)\n",
      "Y shape: (112120, 14)\n",
      "Positives per class: {'Atelectasis': 11559, 'Cardiomegaly': 2776, 'Effusion': 13317, 'Infiltration': 19894, 'Mass': 5782, 'Nodule': 6331, 'Pneumonia': 1431, 'Pneumothorax': 5302, 'Consolidation': 4667, 'Edema': 2303, 'Emphysema': 2516, 'Fibrosis': 1686, 'Pleural_Thickening': 3385, 'Hernia': 227}\n",
      "Split sizes -> Train: 89644 Val: 11321 Test: 11155\n",
      "Indexed files: 112120\n",
      "Missing among first 20: 0\n",
      "DL sizes -> 89644 11321 11155\n",
      " Starting fresh training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001/100 |  mean AUROC=0.7822 | microF1=0.2021 | macroF1=0.1766 | train_loss=1.1280 | val_loss=1.0732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002/100 |  mean AUROC=0.8059 | microF1=0.2291 | macroF1=0.2007 | train_loss=0.9923 | val_loss=1.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003/100 |  mean AUROC=0.8198 | microF1=0.2399 | macroF1=0.2122 | train_loss=0.9347 | val_loss=0.9865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004/100 |  mean AUROC=0.8248 | microF1=0.2612 | macroF1=0.2259 | train_loss=0.8961 | val_loss=0.9957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005/100 |  mean AUROC=0.8250 | microF1=0.2350 | macroF1=0.2136 | train_loss=0.8649 | val_loss=0.9901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006/100 |  mean AUROC=0.8291 | microF1=0.2473 | macroF1=0.2178 | train_loss=0.8595 | val_loss=0.9713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007/100 |  mean AUROC=0.8136 | microF1=0.2347 | macroF1=0.2272 | train_loss=0.8544 | val_loss=1.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008/100 |  mean AUROC=0.8335 | microF1=0.2740 | macroF1=0.2496 | train_loss=0.8508 | val_loss=1.0056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009/100 |  mean AUROC=0.8251 | microF1=0.2457 | macroF1=0.2223 | train_loss=0.8520 | val_loss=1.0081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010/100 |  mean AUROC=0.8063 | microF1=0.2472 | macroF1=0.2213 | train_loss=0.8310 | val_loss=1.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011/100 |  mean AUROC=0.8296 | microF1=0.2713 | macroF1=0.2428 | train_loss=0.8529 | val_loss=1.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 012/100 |  mean AUROC=0.8170 | microF1=0.2195 | macroF1=0.2014 | train_loss=0.8580 | val_loss=1.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013/100 |  mean AUROC=0.8213 | microF1=0.2405 | macroF1=0.2238 | train_loss=0.8623 | val_loss=1.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 014/100 |  mean AUROC=0.8354 | microF1=0.2619 | macroF1=0.2424 | train_loss=0.8748 | val_loss=0.9833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015/100 |  mean AUROC=0.8295 | microF1=0.2707 | macroF1=0.2383 | train_loss=0.8636 | val_loss=0.9984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016/100 |  mean AUROC=0.8188 | microF1=0.2414 | macroF1=0.2205 | train_loss=0.8755 | val_loss=1.0491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 017/100 |  mean AUROC=0.8277 | microF1=0.2495 | macroF1=0.2388 | train_loss=0.8642 | val_loss=0.9976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 018/100 |  mean AUROC=0.8265 | microF1=0.2424 | macroF1=0.2312 | train_loss=0.8699 | val_loss=1.0046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019/100 |  mean AUROC=0.8296 | microF1=0.2498 | macroF1=0.2189 | train_loss=0.8768 | val_loss=1.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020/100 |  mean AUROC=0.8349 | microF1=0.2769 | macroF1=0.2510 | train_loss=0.8577 | val_loss=1.0828\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 021/100 |  mean AUROC=0.8131 | microF1=0.2395 | macroF1=0.2205 | train_loss=0.8752 | val_loss=1.0627\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022/100 |  mean AUROC=0.8309 | microF1=0.2446 | macroF1=0.2325 | train_loss=0.8640 | val_loss=1.0302\n",
      "Early stopping at epoch 22 (best=0.8354)\n",
      " Saved BEST: densenet121_best.pt (epoch=22, best_auc=0.8354)\n",
      " Loaded BEST from epoch 22 (best_auc=0.8354)\n",
      "[TEST] mean AUROC=0.8367 | microF1=0.2654 | macroF1=0.2401\n",
      "Per-class AUROC: {'Atelectasis': 0.8198790424048755, 'Cardiomegaly': 0.8958375221038196, 'Effusion': 0.8881537971582047, 'Infiltration': 0.7060026450977478, 'Mass': 0.8461093517698364, 'Nodule': 0.7895330249974442, 'Pneumonia': 0.761585859477212, 'Pneumothorax': 0.8913858200105019, 'Consolidation': 0.8085117157629805, 'Edema': 0.9063445855614974, 'Emphysema': 0.9049632597962061, 'Fibrosis': 0.8102373210725509, 'Pleural_Thickening': 0.8110970539528977, 'Hernia': 0.8741126422382295}\n"
     ]
    }
   ],
   "source": [
    "# ========================== SETUP ==========================\n",
    "# !pip install -q pandas torch torchvision scikit-learn tqdm\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n",
    "\n",
    "# ========================== EARLY STOPPING ==========================\n",
    "class EarlyStopping:\n",
    "    \"\"\"\n",
    "    Early stop når val_metric ikke forbedres innen 'patience' epoker.\n",
    "    mode='max' brukes for metrikker som AUROC/F1 (vi vil opp),\n",
    "    mode='min' for loss (vi vil ned).\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=8, min_delta=0.0, mode='max', restore_best=True):\n",
    "        assert mode in ('max', 'min')\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.restore_best = restore_best\n",
    "\n",
    "        self.best = None\n",
    "        self.num_bad = 0\n",
    "        self.should_stop = False\n",
    "        self.best_state = None\n",
    "\n",
    "    def _is_better(self, current, best):\n",
    "        if self.mode == 'max':\n",
    "            return current > best + self.min_delta\n",
    "        else:\n",
    "            return current < best - self.min_delta\n",
    "\n",
    "    def step(self, current, model=None):\n",
    "        if self.best is None:\n",
    "            self.best = current\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False  # ikke stoppe første gang\n",
    "\n",
    "        if self._is_better(current, self.best):\n",
    "            self.best = current\n",
    "            self.num_bad = 0\n",
    "            if model is not None:\n",
    "                self.best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "            return False\n",
    "        else:\n",
    "            self.num_bad += 1\n",
    "            if self.num_bad >= self.patience:\n",
    "                self.should_stop = True\n",
    "                return True\n",
    "            return False\n",
    "\n",
    "# ========================== 1) PATHS ==========================\n",
    "CSV_PATH = Path(\"./chest-xray/Data_Entry_2017.csv\")    # <- change if needed\n",
    "BASE     = Path(\"./chest-xray\")                        # contains images_001, images_002, ...\n",
    "assert CSV_PATH.exists(), f\"CSV not found: {CSV_PATH}\"\n",
    "assert BASE.exists(), f\"Base images folder not found: {BASE}\"\n",
    "\n",
    "# ========================== 2) LOAD CSV ==========================\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(\"df shape:\", df.shape)\n",
    "\n",
    "# ========================== 3) LABELS + MULTI-HOT ==========================\n",
    "LABELS = [\n",
    "    'Atelectasis','Cardiomegaly','Effusion','Infiltration','Mass','Nodule',\n",
    "    'Pneumonia','Pneumothorax','Consolidation','Edema','Emphysema',\n",
    "    'Fibrosis','Pleural_Thickening','Hernia'\n",
    "]\n",
    "\n",
    "def to_multi_hot(lbl_str: str):\n",
    "    y = np.zeros(len(LABELS), dtype=np.float32)\n",
    "    if isinstance(lbl_str, str) and lbl_str != \"No Finding\":\n",
    "        for t in lbl_str.split(\"|\"):\n",
    "            if t in LABELS:\n",
    "                y[LABELS.index(t)] = 1.0\n",
    "    return y\n",
    "\n",
    "Y = np.stack([to_multi_hot(s) for s in df[\"Finding Labels\"].astype(str)], axis=0)\n",
    "print(\"Y shape:\", Y.shape)\n",
    "print(\"Positives per class:\", dict(zip(LABELS, Y.sum(axis=0).astype(int))))\n",
    "\n",
    "# ========================== 4) PATIENT-LEVEL SPLIT (80/10/10) ==========================\n",
    "def stable_bucket(pid: str) -> int:\n",
    "    return int(hashlib.md5(pid.encode()).hexdigest(), 16) % 10\n",
    "\n",
    "df[\"Patient ID\"] = df[\"Patient ID\"].astype(str)\n",
    "bucket = df[\"Patient ID\"].map(stable_bucket)\n",
    "train_df = df[bucket < 8].reset_index(drop=True)\n",
    "val_df   = df[bucket == 8].reset_index(drop=True)\n",
    "test_df  = df[bucket == 9].reset_index(drop=True)\n",
    "print(\"Split sizes -> Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n",
    "\n",
    "# ========================== 5) INDEX FILES ACROSS SHARDS ==========================\n",
    "# Layout forventet: BASE / images_XXX / images / *.png\n",
    "name_to_path = {}\n",
    "for p in BASE.glob(\"images_*/images/*.png\"):\n",
    "    name_to_path[p.name] = str(p)\n",
    "\n",
    "print(\"Indexed files:\", len(name_to_path))\n",
    "first20 = df[\"Image Index\"].head(20).tolist()\n",
    "missing20 = [n for n in first20 if n not in name_to_path]\n",
    "print(\"Missing among first 20:\", len(missing20))\n",
    "if missing20:\n",
    "    print(\"Example missing:\", missing20[:5])\n",
    "\n",
    "# ========================== 6) DATASET / DATALOADERS ==========================\n",
    "IMG_SIZE = 384\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize(int(IMG_SIZE*1.1)),\n",
    "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.CenterCrop(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def row_to_multi_hot_tensor(row):\n",
    "    return torch.tensor(to_multi_hot(row[\"Finding Labels\"]), dtype=torch.float32)\n",
    "\n",
    "class ChestXray(Dataset):\n",
    "    def __init__(self, df, index_map, tfm):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.idx = index_map\n",
    "        self.tfm = tfm\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.df.iloc[i]\n",
    "        fname = r[\"Image Index\"]\n",
    "        img_path = self.idx.get(fname)\n",
    "        if img_path is None:\n",
    "            raise FileNotFoundError(f\"Image not indexed: {fname}\")\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = self.tfm(img)\n",
    "        y = row_to_multi_hot_tensor(r)\n",
    "        return x, y\n",
    "\n",
    "train_ds = ChestXray(train_df, name_to_path, train_tfms)\n",
    "val_ds   = ChestXray(val_df,   name_to_path, val_tfms)\n",
    "test_ds  = ChestXray(test_df,  name_to_path, val_tfms)\n",
    "\n",
    "BATCH_SIZE  = 64\n",
    "NUM_WORKERS = 2\n",
    "PIN_MEMORY  = (DEVICE == \"cuda\")\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "val_dl   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "test_dl  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False,\n",
    "                      num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY)\n",
    "\n",
    "print(\"DL sizes ->\", len(train_ds), len(val_ds), len(test_ds))\n",
    "\n",
    "# ========================== 7) MODEL ==========================\n",
    "model = models.densenet121(weights=models.DenseNet121_Weights.IMAGENET1K_V1)\n",
    "in_features = model.classifier.in_features\n",
    "model.classifier = nn.Linear(in_features, len(LABELS))\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ========================== 8) LOSS (pos_weight fra TRAIN) ==========================\n",
    "train_multi = np.vstack(train_df[\"Finding Labels\"].astype(str).map(\n",
    "    lambda s: np.array(to_multi_hot(s), dtype=np.float32)\n",
    ").values)\n",
    "pos = train_multi.sum(axis=0)    # per-klasse positives i TRAIN\n",
    "N = len(train_df)\n",
    "pos = np.clip(pos, 1.0, None)    # unngå div-by-zero\n",
    "pos_weight = torch.tensor((N - pos) / pos, dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "# ========================== 9) EVALUATION HELPERS ==========================\n",
    "def evaluate_with_loss(model, loader, criterion, threshold=0.5):\n",
    "    model.eval()\n",
    "    total_loss, total_n = 0.0, 0\n",
    "    ys, ps = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_n    += xb.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(logits)\n",
    "            ys.append(yb.cpu()); ps.append(probs.cpu())\n",
    "\n",
    "    val_loss = total_loss / max(total_n, 1)\n",
    "\n",
    "    ys = torch.cat(ys, 0).numpy()\n",
    "    ps = torch.cat(ps, 0).numpy()\n",
    "\n",
    "    # AUROC per klasse\n",
    "    aurocs = []\n",
    "    for c in range(len(LABELS)):\n",
    "        y_c, p_c = ys[:, c], ps[:, c]\n",
    "        try:\n",
    "            aurocs.append(roc_auc_score(y_c, p_c))\n",
    "        except ValueError:\n",
    "            aurocs.append(np.nan)\n",
    "    mean_auc = float(np.nanmean(aurocs))\n",
    "\n",
    "    preds = (ps >= threshold).astype(\"int32\")\n",
    "    micro_f1 = f1_score(ys, preds, average=\"micro\", zero_division=0)\n",
    "    macro_f1 = f1_score(ys, preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "    return val_loss, mean_auc, dict(zip(LABELS, aurocs)), micro_f1, macro_f1\n",
    "\n",
    "# Wrapper så TEST-delen funker\n",
    "def evaluate(model, loader, threshold=0.5):\n",
    "    _, mean_auc, per_cls, micro_f1, macro_f1 = evaluate_with_loss(model, loader, criterion, threshold)\n",
    "    return mean_auc, per_cls, micro_f1, macro_f1\n",
    "\n",
    "# ========================== 10) TRAIN with CHECKPOINTS ==========================\n",
    "best_path = \"densenet121_best.pt\"\n",
    "last_path = \"densenet121_last.pt\"\n",
    "RESUME    = False\n",
    "EPOCHS    = 100\n",
    "\n",
    "def make_ckpt(epoch, best_auc, scheduler=None):\n",
    "    return {\n",
    "        \"epoch\": epoch,\n",
    "        \"best_auc\": best_auc,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"scheduler_state\": (scheduler.state_dict() if scheduler is not None else None),\n",
    "        \"scaler_state\": scaler.state_dict(),\n",
    "    }\n",
    "\n",
    "def save_last(epoch, best_auc, scheduler=None):\n",
    "    torch.save(make_ckpt(epoch, best_auc, scheduler), last_path)\n",
    "    print(f\" Saved last: {last_path} (epoch={epoch})\")\n",
    "\n",
    "def save_best(epoch, best_auc, scheduler=None):\n",
    "    torch.save(make_ckpt(epoch, best_auc, scheduler), best_path)\n",
    "    print(f\" Saved BEST: {best_path} (epoch={epoch}, best_auc={best_auc:.4f})\")\n",
    "\n",
    "# Opprett scheduler på forhånd\n",
    "steps_per_epoch = len(train_dl)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer, max_lr=5e-4, steps_per_epoch=steps_per_epoch, epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# Resume (valgfritt)\n",
    "start_epoch = 1\n",
    "best_auc = -1.0\n",
    "if RESUME and os.path.exists(last_path):\n",
    "    ckpt = torch.load(last_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    optimizer.load_state_dict(ckpt[\"optimizer_state\"])\n",
    "    if ckpt.get(\"scheduler_state\") is not None:\n",
    "        scheduler.load_state_dict(ckpt[\"scheduler_state\"])\n",
    "    scaler.load_state_dict(ckpt[\"scaler_state\"])\n",
    "    start_epoch = ckpt[\"epoch\"] + 1\n",
    "    best_auc    = ckpt.get(\"best_auc\", best_auc)\n",
    "    print(f\"   Resuming from epoch {start_epoch} (best_auc={best_auc:.4f})\")\n",
    "else:\n",
    "    print(\" Starting fresh training\")\n",
    "\n",
    "# Early stopping\n",
    "early_stopper = EarlyStopping(patience=8, min_delta=1e-3, mode='max', restore_best=True)\n",
    "\n",
    "for ep in range(start_epoch, EPOCHS + 1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    n_seen = 0\n",
    "\n",
    "    for i, (xb, yb) in enumerate(tqdm(train_dl, desc=f\"Epoch {ep}/{EPOCHS}\", leave=False)):\n",
    "        xb = xb.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE==\"cuda\")):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        # (valgfritt) gradient clipping:\n",
    "        # scaler.unscale_(optimizer)\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        scheduler.step()  # OneCycleLR per batch\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "        n_seen += xb.size(0)\n",
    "\n",
    "    train_loss = running_loss / max(n_seen, 1)\n",
    "\n",
    "    # --- VAL ---\n",
    "    val_loss, val_mean_auc, _, val_micro_f1, val_macro_f1 = evaluate_with_loss(model, val_dl, criterion)\n",
    "    print(f\"Epoch {ep:03d}/{EPOCHS} | \"\n",
    "          f\" mean AUROC={val_mean_auc:.4f} | microF1={val_micro_f1:.4f} | macroF1={val_macro_f1:.4f} | \"\n",
    "          f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f}\")\n",
    "\n",
    "    # Early stop på AUROC\n",
    "    if early_stopper.step(val_mean_auc, model=model):\n",
    "        print(f\"Early stopping at epoch {ep} (best={early_stopper.best:.4f})\")\n",
    "        if early_stopper.restore_best and early_stopper.best_state is not None:\n",
    "            model.load_state_dict(early_stopper.best_state)\n",
    "            model.to(DEVICE)\n",
    "        # valgfritt: lagre beste etter restore\n",
    "        save_best(ep, early_stopper.best, scheduler)\n",
    "        break\n",
    "\n",
    "    # Lagre \"last\" hver epoch\n",
    "    #save_last(ep, best_auc, scheduler)\n",
    "\n",
    "    # # Lagre \"best\" når forbedret\n",
    "    # if val_mean_auc > best_auc:\n",
    "    #     best_auc = val_mean_auc\n",
    "    #     save_best(ep, best_auc, scheduler)\n",
    "\n",
    "# ========================== 11) TEST ==========================\n",
    "if Path(best_path).exists():\n",
    "    ckpt = torch.load(best_path, map_location=DEVICE)\n",
    "    model.load_state_dict(ckpt[\"model_state\"])\n",
    "    model.to(DEVICE)\n",
    "    print(f\" Loaded BEST from epoch {ckpt['epoch']} (best_auc={ckpt['best_auc']:.4f})\")\n",
    "else:\n",
    "    print(\"⚠ BEST checkpoint not found, using last model weights in memory.\")\n",
    "\n",
    "test_mean_auc, test_per_cls, test_micro_f1, test_macro_f1 = evaluate(model, test_dl)\n",
    "print(f\"[TEST] mean AUROC={test_mean_auc:.4f} | microF1={test_micro_f1:.4f} | macroF1={test_macro_f1:.4f}\")\n",
    "print(\"Per-class AUROC:\", {k: (None if np.isnan(v) else float(v)) for k, v in test_per_cls.items()})\n",
    "\n",
    "# ========================== 12) (Optional) SINGLE-IMAGE PREDICT ==========================\n",
    "@torch.no_grad()\n",
    "def predict_image(img_path, threshold=0.5, top_k=5):\n",
    "    model.eval()\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = val_tfms(img).unsqueeze(0).to(DEVICE)\n",
    "    probs = torch.sigmoid(model(x)).squeeze(0).cpu().numpy()\n",
    "    pred_labels = [LABELS[i] for i, p in enumerate(probs) if p >= threshold]\n",
    "    top_idx = np.argsort(-probs)[:top_k]\n",
    "    top = [(LABELS[i], float(probs[i])) for i in top_idx]\n",
    "    return dict(zip(LABELS, map(float, probs))), pred_labels, top\n",
    "\n",
    "# Example:\n",
    "# img_example = \"/chest-xray/images_001/images/00001335_006.png\"\n",
    "# probs, preds, top5 = predict_image(img_example, threshold=0.5, top_k=5)\n",
    "# print(\"Predicted (>=0.5):\", preds)\n",
    "# print(\"Top-5:\", top5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91254a30-c58a-4c1b-8cb0-0b3e17769b02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
